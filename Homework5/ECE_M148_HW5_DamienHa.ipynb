{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d56d3a-6629-40f5-8cbf-d95b3d60fe64",
   "metadata": {},
   "source": [
    "# ECE M148, Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1a9b5-117e-4960-a2a9-f2f11c6851db",
   "metadata": {},
   "source": [
    "## Damien Ha, UID 905539967"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57c685-2dbd-4b6b-90a6-f3a467b83d7f",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66256d-208a-430b-8721-2792bf598aa0",
   "metadata": {},
   "source": [
    "#### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da15b95-2e5f-4da7-b94d-484921c96b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Index Gain (Humidity): 0.05804988662131538\n",
      "Gini Index Gain (Wind): 0.16326530612244905\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Day': list(range(1, 15)),\n",
    "    'Humidity': ['High', 'High', 'Normal', 'Normal', 'High', 'Normal', 'High', 'Normal', 'High', 'High', 'High', 'High', 'High', 'Normal'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Strong', 'Strong'],\n",
    "    'Play Tennis': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'No']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def calculate_gini_index(groups, classes):\n",
    "    total_instances = sum(len(group) for group in groups)\n",
    "    gini_index = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            proportion = [row[-1] for row in group].count(class_val) / size\n",
    "            score += proportion * proportion\n",
    "        gini_index += (1.0 - score) * (size / total_instances)\n",
    "    return gini_index\n",
    "\n",
    "def calculate_gini_index_gain(data, feature):\n",
    "    current_gini_index = calculate_gini_index([data.values], df['Play Tennis'].unique())\n",
    "    \n",
    "    unique_values = df[feature].unique()\n",
    "    feature_index = df.columns.get_loc(feature)\n",
    "    \n",
    "    weighted_gini_index = 0.0\n",
    "    for value in unique_values:\n",
    "        subset = data[data[feature] == value]\n",
    "        subset_gini_index = calculate_gini_index([subset.values], df['Play Tennis'].unique())\n",
    "        weighted_gini_index += len(subset) / len(data) * subset_gini_index\n",
    "    \n",
    "    gini_index_gain = current_gini_index - weighted_gini_index\n",
    "    return gini_index_gain\n",
    "\n",
    "gini_index_humidity = calculate_gini_index_gain(df, 'Humidity')\n",
    "gini_index_wind = calculate_gini_index_gain(df, 'Wind')\n",
    "\n",
    "print(\"Gini Index Gain (Humidity):\", gini_index_humidity)\n",
    "print(\"Gini Index Gain (Wind):\", gini_index_wind)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a614a-46ea-4c77-b5a1-a7662e0c582a",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Wind, based on the results above, provides the best Gini Index Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7750fd9-bc22-4005-8c39-189f8dfaa202",
   "metadata": {},
   "source": [
    "#### Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dca7614-14d7-4c07-8961-7ff990ddd5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain (Humidity): 0.09027634939276485\n",
      "Information Gain (Wind): 0.2578314624597723\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_entropy(data):\n",
    "    total_instances = len(data)\n",
    "    class_counts = data['Play Tennis'].value_counts()\n",
    "    entropy = 0.0\n",
    "    for count in class_counts:\n",
    "        proportion = count / total_instances\n",
    "        entropy -= proportion * math.log2(proportion)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(data, feature):\n",
    "    current_entropy = calculate_entropy(data)\n",
    "    \n",
    "    unique_values = df[feature].unique()\n",
    "    feature_index = df.columns.get_loc(feature)\n",
    "    \n",
    "    weighted_entropy = 0.0\n",
    "    for value in unique_values:\n",
    "        subset = data[data[feature] == value]\n",
    "        subset_entropy = calculate_entropy(subset)\n",
    "        weighted_entropy += len(subset) / len(data) * subset_entropy\n",
    "    \n",
    "    information_gain = current_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "information_gain_humidity = calculate_information_gain(df, 'Humidity')\n",
    "information_gain_wind = calculate_information_gain(df, 'Wind')\n",
    "\n",
    "print(\"Information Gain (Humidity):\", information_gain_humidity)\n",
    "print(\"Information Gain (Wind):\", information_gain_wind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c3291-c2e7-4df3-811f-5993e31dee52",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Wind is still the best feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97253973-6e6b-4125-b5d0-d8555241138f",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f317888e-c8a5-404e-98f4-d68f4e9007f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Assignments (after one iteration): [0 0 0 1 1 1 1 1 0]\n",
      "New Cluster Centers (after one iteration):\n",
      " [[2 2]\n",
      " [5 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 1], [2, 4], [4, 1], [6, 3], [5, 7], [8, 1], [4, 4], [3, 6], [3, 3]])\n",
    "centers = np.array([[2, 2], [5, 5]])\n",
    "\n",
    "distances = np.zeros((data.shape[0], centers.shape[0]))\n",
    "for i in range(centers.shape[0]):\n",
    "    distances[:, i] = np.linalg.norm(data - centers[i], axis=1)\n",
    "\n",
    "cluster_assignments = np.argmin(distances, axis=1)\n",
    "print(\"Cluster Assignments (after one iteration):\", cluster_assignments)\n",
    "\n",
    "new_centers = np.zeros_like(centers)\n",
    "for i in range(centers.shape[0]):\n",
    "    new_centers[i] = np.mean(data[cluster_assignments == i], axis=0)\n",
    "\n",
    "print(\"New Cluster Centers (after one iteration):\\n\", new_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fae0a-37b0-4959-9d29-23db486c1838",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa0d1c-09dc-4d59-8dc5-6a02de5387e5",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "The circled points appear to be the support vectors as we can visually see that the circled points are where + and - points are seperated. Removing one of the non-circled points therefore shouldn't have any effect on the decision boundary, none of them are close to it and the support vectors would remain.\n",
    "\n",
    "#### Part B\n",
    "A hard margin SVM seeks a perfect separation of classes and assumes they can be seperated linearly, while a soft margin SVM allows for some misclassifications and includes a margin of error to handle overlapping or noisy data points\n",
    "\n",
    "#### Part C\n",
    "5, the remaining 3 +'s would become support vectors as they'd be equally close to the decision boundary, and the orginal 2 -'s would remain giving us 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb3fe3-b661-445c-bcdd-e1698c460484",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88c8e4-0b84-40a4-a449-112ab2190f3f",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "For the first expression:\n",
    "\n",
    "$P(Y = i \\mid X) = \\frac{e^{\\beta_{0i} + \\beta_{1i}X}}{1 + \\sum_{j=1}^{K-1}e^{\\beta_{0i} + \\beta_{1i}X}}, 1 \\leq i \\leq K-1$\n",
    "\n",
    "$= \\frac{e^{\\beta_{0i} + \\beta_{1i}X}e^{\\beta_{0K} + \\beta_{1K}X}}{e^{\\beta_{0K} + \\beta_{1K}X} + \\sum_{j=1}^{K-1}e^{\\beta_{0i} + \\beta_{1i}X}}$\n",
    "\n",
    "$= e^{\\beta_{0i} + \\beta_{1i}X} \\cdot \\frac{e^{\\beta_{0K} + \\beta_{1K}X}}{e^{\\beta_{0K} + \\beta_{1K}X} + \\sum_{j=1}^{K-1}e^{\\beta_{0i} + \\beta_{1i}X}}$\n",
    "\n",
    "$= e^{\\beta_{0i} + \\beta_{1i}X} \\cdot a \\ constant$\n",
    "\n",
    "For the second expression:\n",
    "\n",
    "$P(Y = i \\mid X) = \\frac{e^{\\hat{\\beta_{0i}} + \\hat{\\beta_{1i}X}}}{1 + \\sum_{j=1}^{K-1}e^{\\hat{\\beta_{0i}} + \\hat{\\beta_{1i}X}}}, 1 \\leq i \\leq K$\n",
    "\n",
    "$= \\frac{e^{\\hat{\\beta_{0i}} + \\hat{\\beta_{1i}X}}}{e^{\\hat{\\beta_{0K}} + \\hat{\\beta_{1K}X \\cdot C}}}$\n",
    "\n",
    "Subbing in and canceling terms we get\n",
    "\n",
    "$= \\frac{e^{\\hat{\\beta_{0i}} + \\hat{\\beta_{1i}}Xe^{\\beta_{0i} + \\beta_{1i}X}}}{e^{\\hat{\\beta_{0K}} + \\hat{\\beta_{1K}}X}}$\n",
    "\n",
    "$= e^{\\hat{\\beta_{0i}} + \\hat{\\beta_{1i}}X} e^{\\beta_{0i} + \\beta_{1i}X}$\n",
    "\n",
    "$= e^{\\hat{\\beta_{0i}} + \\hat{\\beta_{1i}}X + \\beta_{0i} + \\beta_{1i}X}$\n",
    "\n",
    "$= e^{\\beta_{0i} + \\beta_{1i}X}$\n",
    "\n",
    "The two are equivalent\n",
    "\n",
    "#### Part B\n",
    "\n",
    "$P(Y = 1 \\mid X) = \\frac{e^{-0.2 + 0.06 \\cdot 5}}{1 + e^{-0.2 + 0.06 \\cdot 5} + e^{-0.2 + 0.04 \\cdot 5}} = 0.146$\n",
    "\n",
    "$P(Y = 2 \\mid X) = \\frac{e^{0.2 + 0.04 \\cdot 5}}{1 + e^{-0.2 + 0.06 \\cdot 5} + e^{-0.2 + 0.04 \\cdot 5}} = 0.252$\n",
    "\n",
    "$P(Y = 3 \\mid X) = \\frac{e^{-0.3 + 0.5 \\cdot 5}}{1 + e^{-0.2 + 0.06 \\cdot 5} + e^{-0.2 + 0.04 \\cdot 5}} = 0.602$\n",
    "\n",
    "Class 3, as it has the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b40f7e6-af54-426d-b413-0cb16977a42f",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaa937-af43-415c-90c0-d374e2458690",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "False, we would actually want to pick the feature whose split minimizes MSE, so there is a lower error\n",
    "\n",
    "#### Part B\n",
    "False, K-means is sensitive to the initial points/centroids, so different initializations can lead to drastically different clusters and different solutions\n",
    "\n",
    "#### Part C\n",
    "False, its goal is to combine similar clusters and then create a hierarchy of clusters which is not necessarily minimizing distortion\n",
    "\n",
    "#### Part D\n",
    "False, larger $\\lambda$ implies a narrower margin due to a higher penalty\n",
    "\n",
    "#### Part E\n",
    "True, by bootstrapping and considering a subset of features for each node, we reduce the variance and generalize the performance, avoiding overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
